Year,Sources,Name,Authors,First Author,Chinese/English,Abstract,Venues,doi,Citation,Id,Keywords
2020,Journal of Usability Studies,Practices and Challenges of Using Think-Aloud Protocols in Industry: An International Survey,"Mingming Fan, Serina Shi, Khai N Truong",Mingming Fan,English,"Think-aloud protocols are one of the classic methods often taught in universities for training UX designers and researchers. Although previous research reported how these protocols were used in industry, the findings were typically based on the practices of a small number of professionals in specific geographic regions or on studies conducted years ago. As UX practices continuously evolve to address new challenges emerging in industry, it is important to understand the challenges faced by current UX practitioners around the world when using think-aloud protocols. Such an understanding is beneficial for UX professionals to reflect on and learn from the UX community¡¯s practices. It is also invaluable for academic researchers and educators to understand the challenges faced by professionals when carrying out the protocols in a wide range of practical contexts and to better explore methods to address these challenges. We conducted an international survey study with UX professionals in various sized companies around the world. We found that think-aloud protocols are widely and almost equally used in controlled lab studies and remote usability testing; concurrent protocols are more popular than retrospective protocols. Most UX practitioners probe participants during test sessions, explicitly request them to verbalize particular types of content, and do not administer practice sessions. The findings also offer insights on practices and challenges in analyzing think-aloud sessions. In sum, UX practitioners often deal with the tension between validity and efficiency in their analysis and demand better fast-paced and reliable analysis methods than?¡­",Article,http://uxpajournal.org/wp-content/uploads/sites/7/pdf/JUS_Fan_Feb2020.pdf,75,fan2020practices,"Think-aloud protocols, usability test, user experience, industry practices and challenges, international survey"
2020,IEEE Transactions on Visualization and Computer Graphics,Chartseer: Interactive steering exploratory visual analysis with machine intelligence,"Jian Zhao, Mingming Fan, Mi Feng",Jian Zhao,English,"During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9174891/,42,zhao2020chartseer,"Exploratory visual analysis, interactive steering, visualization recommendation, machine learning"
2021,IEEE Transactions on Software Engineering,Accessible or not? an empirical investigation of Android app accessibility,"Sen Chen, Chunyang Chen, Lingling Fan, Mingming Fan, Xian Zhan, Yang Liu",Sen Chen,English,"Mobile apps provide new opportunities to people with disabilities to act independently in the world. Following the law of the US, EU, mobile OS vendors such as Google and Apple have included accessibility features in their mobile systems and provide a set of guidelines and toolsets for ensuring mobile app accessibility. Motivated by this trend, researchers have conducted empirical studies by using the inaccessibility issue rate of each page (i.e., screen level) to represent the characteristics of mobile app accessibility. However, there still lacks an empirical investigation directly focusing on the issues themselves (i.e., issue level) to unveil more fine-grained findings, due to the lack of an effective issue detection method and a relatively comprehensive dataset of issues. To fill in this literature gap, we first propose an automated app page exploration tool, named Xbot, to facilitate app accessibility testing and?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9525343/,39,chen2021accessible,"Mobile accessibility, empirical study, automated accessibility testing, Android app, Xbot"
2022,Proceedings of the ACM on Human-Computer Interaction,Human-ai collaboration for ux evaluation: Effects of explanation and synchronization,"Mingming Fan, Xianyou Yang, TszTung Yu, Q Vera Liao, Jian Zhao",Mingming Fan,English,"Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to the two ways humans and AI collaborate: synchronously and asynchronously. We iteratively designed a tool-AI Assistant-with four versions of UIs corresponding to the two levels of explanations (with/without) and synchronization (sync/async). By adopting a hybrid wizard-of-oz approach to simulating an AI with reasonable?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3512943,30,fan2022human," human-AI collaboration, user experience(UX), AI-assisted UX evaluation,explainable AI, intelligent user interface(UI) design, synchronization, explanation, think-aloud usability test"
2021,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems?¡­,¡°I Choose Assistive Devices That Save My Face¡± A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China,"Franklin Mingzhe Li, Di Laura Chen, Mingming Fan, Khai N Truong",Franklin Mingzhe Li,English," Despite the potential benefits of assistive technologies (ATs) for people with various disabilities, only around 7% of Chinese with disabilities have had an opportunity to use ATs. Even for those who have used ATs, the abandonment rate was high. Although China has the world¡¯s largest population with disabilities, prior research exploring how ATs are used and perceived, and why ATs are abandoned have been conducted primarily in North America and Europe. In this paper, we present an interview study conducted in China with 26 people with various disabilities to understand their practices, challenges, perceptions, and misperceptions of using ATs. From the study, we learned about factors that influence AT adoption practices (e.g., misuse of accessible infrastructure, issues with replicating existing commercial ATs), challenges using ATs in social interactions (e.g., Chinese stigma), and misperceptions about ATs?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3411764.3445321,27,li2021choose,"Assistive technology, Accessibility, People with disabilities, Qualitative study, Interview, China, Misperceptions "
2020,ACM Transactions on Interactive Intelligent Systems (TiiS),Automatic Detection of Usability Problem Encounters in Think-Aloud Sessions,"Mingming Fan, Yue Li, Khai N Truong",Mingming Fan,English,"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users¡¯ verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users¡¯ verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3385732,26,fan2020automatic,"Think aloud, usability problem, verbalization, speech features, machine learning, user experience (UX), AI-assisted UX analysis method"
2020,Proceedings of the 22nd International ACM SIGACCESS Conference on Computers?¡­,Eyelid gestures on mobile devices for people with motor impairments,"Mingming Fan, Zhen Li, Franklin Mingzhe Li",Mingming Fan,English," Eye-based interactions for people with motor impairments have often used clunky or specialized equipment (e.g., eye-trackers with non-mobile computers) and primarily focused on gaze and blinks. However, two eyelids can open and close for different duration in different orders to form various eyelid gestures. We take a first step to design, detect, and evaluate a set of eyelid gestures for people with motor impairments on mobile devices. We present an algorithm to detect nine eyelid gestures on smartphones in real-time and evaluate it with twelve able-bodied people and four people with severe motor impairments in two studies. The results of the study with people with motor-impairments show that the algorithm can detect the gestures with .76 and .69 overall accuracy in user-dependent and user-independent evaluations. Moreover, we design and evaluate a gesture mapping scheme allowing for navigating mobile?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3373625.3416987,25,fan2020eyelid,None
2021,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems?¡­,Older adults¡¯ think-aloud verbalizations and speech features for identifying user experience problems,"Mingming Fan, Qiwen Zhao, Vinita Tibdewal",Mingming Fan,English,"Subtle patterns in users¡¯ think-aloud (TA) verbalizations and speech features are shown to be telltale signs of User Experience (UX) problems. However, such patterns were uncovered among young adults. Whether such patterns apply for older adults remains unknown. We conducted TA usability testing with older adults using physical and digital products. We analyzed their verbalizations, extracted speech features, identified UX problems, and uncovered the patterns that indicate UX problems. Our results show that when older adults encounter problems, their verbalizations tend to include observations (remarks), negations, question words and words with negative sentiments; and their voices tend to include high loudness, high pitch and high speech rate. We compare these subtle patterns with those of young adults uncovered in recent studies and discuss the implications of these patterns for the design of Human?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3411764.3445680,24,fan2021older,"older adults, elderly, seniors, think-aloud,  verbalization,  speech features, UX problems, usability testing, remote usability testing, AI-assisted UX analysis, human-AI collaboration for UX analysis "
2021,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems?¡­,Vmirror: Enhancing the interaction with occluded or distant objects in vr with virtual mirrors,"Nianlong Li, Zhengquan Zhang, Can Liu, Zengyao Yang, Yinan Fu, Feng Tian, Teng Han, Mingming Fan",Nianlong Li,English,"Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror¡¯s orientation on users¡¯ performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3411764.3445537,23,li2021vmirror,"Virtual mirror, vMirror, Virtual Reality, VR, target selection, occlusion, out of reach, DOF, raycasting"
2022,Proceedings of the Tenth International Symposium of Chinese CHI,Communication in immersive social virtual reality: A systematic review of 10 years¡¯ studies,"Xiaoying Wei, Xiaofu Jin, Mingming Fan",Xiaoying Wei,English,"As virtual reality (VR) technologies have improved in the past decade, more research has investigated how they could support more effective communication in various contexts to improve collaboration and social connectedness. However, there was no literature to summarize the uniqueness VR provided and put forward guidance for designing social VR applications for better communication. To understand how VR has been designed and used to facilitate communication in different contexts, we conducted a systematic review of the studies investigating communication in social VR in the past ten years by following the PRISMA guidelines. We highlight current practices and challenges and identify research opportunities to improve the design of social VR to better support communication and make social VR more accessible.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3565698.3565767,21,wei2022communication,"Virtual Reality, Social VR, Communication, Evaluation"
2021,IEEE Transactions on Visualization and Computer Graphics,CoUX: collaborative visual analysis of think-aloud usability test videos for digital interfaces,"Ehsan Jahangirzadeh Soure, Emily Kuang, Mingming Fan, Jian Zhao",Ehsan Jahangirzadeh Soure,English,"Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a?¡­",Article,https://ieeexplore.ieee.org/abstract/document/9552211/,21,soure2021coux,"User experience, usability problems, think-aloud, video analysis, machine learning, visual analytics, collaboration"
2020,Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems?¡­,Mouill¨¦: Exploring wetness illusion on fingertips to enhance immersive experience in vr,"Teng Han, Sirui Wang, Sijia Wang, Xiangmin Fan, Jie Liu, Feng Tian, Mingming Fan",Teng Han,English,"Providing users with rich sensations is beneficial to enhance their immersion in Virtual Reality (VR) environments. Wetness is one such imperative sensation that affects users' sense of comfort and helps users adjust grip force when interacting with objects. Researchers have recently begun to explore ways to create wetness illusions, primarily on a user's face or body skin. In this work, we extended this line of research by creating wetness illusion on users' fingertips. We first conducted a user study to understand the effect of thermal and tactile feedback on users' perceived wetness sensation. Informed by the findings, we designed and evaluated a prototype---Mouill¨¦---that provides various levels of wetness illusions on fingertips for both hard and soft items when users squeeze, lift, or scratch it. Study results indicated that users were able to feel wetness with different levels of temperature changes and they were able?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3313831.3376138,19,han2020mouille,"Wetness illusion, virtual reality, prototype, user study"
2022,Proceedings of the ACM on Interactive,Synapse: interactive guidance by demonstration with trial-and-error support for older adults to use smartphone apps,"Xiaofu Jin, Xiaozhu Hu, Xiaoying Wei, Mingming Fan",Xiaofu Jin,English,"As smartphones are widely adopted, mobile applications (apps) are emerging to provide critical services such as food delivery and telemedicine. While bring convenience to everyday life, this trend may create barriers for older adults who tend to be less tech-savvy than young people. In-person or screen sharing support is helpful but limited by the help-givers' availability. Video tutorials can be useful but require users to switch contexts between watching the tutorial and performing the corresponding actions in the app, which is cumbersome to do on a mobile phone. Although interactive tutorials have been shown to be promising, none was designed for older adults. Furthermore, the trial-and-error approach has been shown to be beneficial for older adults, but they often lack support to use the approach. Inspired by both interactive tutorials and trial-and-error approach, we designed an app-independent mobile service?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3550321,15,jin2022synapse,None
2022,In CHI Conference on Human Factors in Computing Systems (CHI'22),From Wow to Why: Guidelines for Creating the Opening of a Data Video with Cinematic Styles,"Xian Xu, Leni Yang, David Yip, Mingming Fan, Zheng Wei, Huamin Qu",Xian Xu,English," Data videos are an increasingly popular storytelling form. The opening of a data video critically influences its success as the opening either attracts the audience to continue watching or bores them to abandon watching. However, little is known about how to create an attractive opening. We draw inspiration from the openings of famous films to facilitate designing data video openings. First, by analyzing over 200 films from several sources, we derived six primary cinematic opening styles adaptable to data videos. Then, we consulted eight experts from the film industry to formulate 28 guidelines. To validate the usability and effectiveness of the guidelines, we asked participants to create data video openings with and without the guidelines, which were then evaluated by experts and the general public. Results showed that the openings designed with the guidelines were perceived to be more attractive, and the?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3491102.3501896,15,xu2022wow,"Visualization, Storytelling, Interview, Lab Study, Data Video, Guideline "
2021,Proceedings of the ACM on Interactive,Douleur: creating pain sensation with chemical stimulant to enhance user experience in virtual reality,"Chutian Jiang, Yanjun Chen, Mingming Fan, Liuping Wang, Luyao Shen, Nianlong Li, Wei Sun, Yu Zhang, Feng Tian, Teng Han",Chutian Jiang,English,"The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3463527,15,jiang2021douleur,"Virtual reality, pain sensation, prototype, capsaicin, user experience, safety education"
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,""" I am the follower, also the boss"": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired","Yan Zhang, Ziang Li, Haole Guo, Luyao Wang, Qihe Chen, Wenjie Jiang, Mingming Fan, Guyue Zhou, Jiangtao Gong",Yan Zhang,English," Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3580884,13,zhang2023follower,"guiding robot, visual impairment, navigation, level of autonomy, machine form, control, trust, safety "
2022,Designing Interactive Systems Conference,"""It Feels Like Being Locked in A Cage"": Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms","Ethan Z. Rong, Mo Morgana Zhou, Zhicong Lu, Mingming Fan",Ethan Z. Rong,English,"Blind or low vision (BLV) people were recently reported to be live streamers on the online platforms that employed content curation algorithms. Recent research uncovered perceived algorithmic biases suppressing the content created by marginalized populations (e.g., people of color, the LGBT+ community, and content creators of lower socioeconomic status). However, little is known about how BLV streamers, as a marginalized population, perceive the effects of the algorithms adopted by live streaming platforms. We interviewed BLV streamers (N=19) of Douyin ¡ª a popular live stream platform in China ¡ª to understand their perceptions of algorithms, perceived challenges, and mitigation strategies. Our findings show the perceived factors contributing to disadvantages under algorithmic evaluation of BLV streamers¡¯ content (e.g., issues with filming and timely interaction with viewers) and perceived algorithmic?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3532106.3533514,13,rong2022feels,"Algorithms, Algorithmic Experience, Perceptions of Algorithms,Accessibility, Individuals with Disabilities & Assistive Technologies,Social Media/Online Communities, Interview"
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,Bridging the generational gap: exploring how virtual reality supports remote communication between grandparents and grandchildren,"Xiaoying Wei, Yizheng Gu, Emily Kuang, Xian Wang, Beiyan Cao, Xiaofu Jin, Mingming Fan",Xiaoying Wei,English,"When living apart, grandparents and grandchildren often use audio-visual communication approaches to stay connected. However, these approaches seldom provide sufficient companionship and intimacy due to a lack of co-presence and spatial interaction, which can be fulfilled by immersive virtual reality (VR). To understand how grandparents and grandchildren might leverage VR to facilitate their remote communication and better inform future design, we conducted a user-centered participatory design study with twelve pairs of grandparents and grandchildren. Results show that VR affords casual and equal communication by reducing the generational gap, and promotes conversation by offering shared activities as bridges for connection. Participants preferred resemblant appearances on avatars for conveying well-being but created ideal selves for gaining playfulness. Based on the results, we contribute eight?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581405,12,wei2023bridging,"inter-generational communication, generational gap, VR, virtual reality, aging, older adults, grandparents, grandchildren "
2022,Proceedings of the Tenth International Symposium of Chinese CHI,"Reducing stress and anxiety in the metaverse: A systematic review of meditation, mindfulness and virtual reality","Xian Wang, Xiaoyu Mo, Mingming Fan, Lik-Hang Lee, Bertram Shi, Pan Hui",Xian Wang,English," Meditation, or mindfulness, is widely used to improve mental health. With the emergence of Virtual Reality technology, many studies have provided evidence that meditation with VR can bring health benefits. However, to our knowledge, there are no guidelines and comprehensive reviews in the literature on how to conduct such research in virtual reality. In order to understand the role of VR technology in meditation and future research opportunities, we conducted a systematic literature review in the IEEE and ACM databases. Our process yielded 19 eligible papers and we conducted a structured analysis. We understand the state-of-art of meditation type, design consideration and VR and technology through these papers and conclude research opportunities and challenges for the future.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3565698.3565781,11,wang2022reducing,"Meditation, Mindfulness, Virtual Reality, Literature Review, Interaction, Metaverse"
2022,In CHI Conference on Human Factors in Computing Systems (CHI'22),"""I Don't Want People to Look At Me Differently"": Designing User-Defined Above-the-Neck Gestures for People with Upper Body Motor Impairments","Xuan Zhao, Mingming Fan, Teng Han",Xuan Zhao,English,"Recent research proposed eyelid gestures for people with upper-body motor impairments (UMI) to interact with smartphones without finger touch. However, such eyelid gestures were designed by researchers. It remains unknown what eyelid gestures people with UMI would want and be able to perform. Moreover, other above-the-neck body parts (e.g., mouth, head) could be used to form more gestures. We conducted a user study in which 17 people with UMI designed above-the-neck gestures for 26 common commands on smartphones. We collected a total of 442 user-defined gestures involving the eyes, the mouth, and the head. Participants were more likely to make gestures with their eyes and preferred gestures that were simple, easy-to-remember, and less likely to draw attention from others. We further conducted a survey (N=24) to validate the usability and acceptance of these user-defined gestures. Results?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3491102.3517552,11,zhao2022don,"people with motor impairments, above-the-neck gestures, user-defined gestures, gesture elicitation "
2022,In CHI Conference on Human Factors in Computing Systems (CHI'22),"¡°I need to be professional until my new team uses emoji, GIFs, or memes first¡±: New Collaborators¡¯ Perspectives on Using Non-Textual Communication in Virtual Workspaces","Esha Shandilya, Mingming Fan, Garreth W Tigwell",Esha Shandilya,English," Virtual workspaces rapidly increased during the COVID-19 pandemic, and for many new collaborators, working remotely was their first introduction to their colleagues. Building rapport is essential for a healthy work environment, and while this can be achieved through non-textual responses within chat-based systems (e.g., emoji, GIF, stickers, memes), those non-textual responses are typically associated with personal relationships and informal settings. We studied the experiences of new collaborators (questionnaire N=49; interview N=14) in using non-textual responses to communicate with unacquainted teams and the effect of non-textual responses on new collaborators¡¯ interpersonal bonds. We found new collaborators selectively and progressively use non-textual responses to establish interpersonal bonds. Moreover, the use of non-textual responses has exposed several limitations when used on various?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3491102.3517514,11,shandilya2022need,"Virtual workspaces,  Computer-mediated Communication,  Non-textual Communication "
2022,Proceedings of the Tenth International Symposium of Chinese CHI,Understanding older adults¡¯ perceptions and challenges in using AI-enabled everyday technologies,"Esha Shandilya, Mingming Fan",Esha Shandilya,English,"Artificial intelligence (AI)-enabled everyday technologies could help address age-related challenges like physical impairments and cognitive decline. While recent research studied older adults¡¯ experiences with specific AI-enabled products (e.g., conversational agents and assistive robots), it remains unknown how older adults perceive and experience current AI-enabled everyday technologies in general, which could impact their adoption of future AI-enabled products. We conducted a survey study (N=41) and semi-structured interviews (N=15) with older adults to understand their experiences and perceptions of AI. We found that older adults were enthusiastic about learning and using AI-enabled products, but they lacked learning avenues. Additionally, they worried when AI-enabled products outwitted their expectations, intruded on their privacy, or impacted their decision-making skills. Therefore, they held mixed?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3565698.3565774,10,shandilya2022understanding,"AI-enabled everyday technologies, Older Adults, Interview, Perceptions"
2021,ACM SIGCHI Conference on Designing Interactive Systems Conference 2021 (DIS?¡­,""" Too old to bank digitally?"": A Survey of Banking Practices and Challenges Among Older Adults in China","Xiaofu Jin, Emily Kuang, Mingming Fan",Xiaofu Jin,English," The banking industry has been integrating digital technologies globally. However, accepting new technologies is challenging in particular for older adults. We focus on older adults¡¯ banking experiences in China, where digital transactions have been growing rapidly, to provide a perspective on how they adapt to this trend. We conducted an online survey with 155 older adults who are 60 or above (M = 70, SD = 9) from 18 provinces to explore their banking practices and challenges. Our results show that older adults conduct banking transactions frequently. However, few do so using digital platforms despite long wait times in physical banks. The main concerns reported by them are about security and usability. Nonetheless, they hold a positive attitude towards digital platforms (e.g., apps, virtual banks). Interestingly, age and gender have significant effects on particular banking behaviors. We discuss our findings in the?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3461778.3462127,10,jin2021too,"older adults, elderly, seniors, aging, banking, electronic payment, accessibility, technology use, digital inclusion, digital equity"
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,Collaboration with conversational AI assistants for UX evaluation: Questions and how to ask them (voice vs. text),"Emily Kuang, Ehsan Jahangirzadeh Soure, Mingming Fan, Jian Zhao, Kristen Shinohara",Emily Kuang,English," AI is promising in assisting UX evaluators with analyzing usability tests, but its judgments are typically presented as non-interactive visualizations. Evaluators may have questions about test recordings, but have no way of asking them. Interactive conversational assistants provide a Q&A dynamic that may improve analysis efficiency and evaluator autonomy. To understand the full range of analysis-related questions, we conducted a Wizard-of-Oz design probe study with 20 participants who interacted with simulated AI assistants via text or voice. We found that participants asked for five categories of information: user actions, user mental model, help from the AI assistant, product and task information, and user demographics. Those who used the text assistant asked more questions, but the question lengths were similar. The text assistant was perceived as significantly more efficient, but both were rated equally in?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581247,9,kuang2023collaboration,"User experience(UX),  UX evaluation, Usability testing, Human-AI collaboration, Conversational assistants "
2023,International Journal of Human¨CComputer Interaction,Understanding how older adults comprehend COVID-19 interactive visualizations via think-aloud protocol,"Mingming Fan, Yiwen Wang, Yuni Xie, Franklin Mingzhe Li, Chunyang Chen",Mingming Fan,English,"Older adults have been hit disproportionally hard by the COVID-19 pandemic. One critical way for older adults to minimize the negative impact of COVID-19 and future pandemics is to stay informed about its latest information, which has been increasingly presented through online interactive visualizations (e.g., live dashboards and websites). Thus, it is imperative to understand how older adults interact with and comprehend online COVID-19 interactive visualizations and what challenges they might encounter to make such visualizations more accessible to older adults. We adopted a user-centered approach by inviting older adults to interact with COVID-19 interactive visualizations while at the same time verbalizing their thought processes using a think-aloud protocol. By analyzing their think-aloud verbalizations, we identified four types of thought processes representing how older adults comprehended the?¡­",Article,https://www.tandfonline.com/doi/abs/10.1080/10447318.2022.2064609,8,fan2023understanding,None
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing,"Zisu Li, Chen Liang, Yuntao Wang, Yue Qin, Chun Yu, Yukang Yan, Mingming Fan, Yuanchun Shi",Zisu Li,English," Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581008,8,li2023enabling,"hand gestures, acoustic sensing, sensor fusion "
2023,IEEE Transactions on Visualization and Computer Graphics,uxSense: Supporting user experience analysis with visualization and computer vision,"Andrea Batch, Yipeng Ji, Mingming Fan, Jian Zhao, Niklas Elmqvist",Andrea Batch,English,"Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",Article,https://ieeexplore.ieee.org/abstract/document/10034833/,8,batch2023uxsense,"Visualization, visual analytics, evaluation, video analytics, machine learning, deep learning, computer vision."
2022,In CHI Conference on Human Factors in Computing Systems (CHI'22),"""Merging Results Is No Easy Task"": An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners","Emily Kuang, Xiaofu Jin, Mingming Fan",Emily Kuang,English,"Analysis is a key part of usability testing where UX practitioners seek to identify usability problems and generate redesign suggestions. Although previous research reported how analysis was conducted, the findings were typically focused on individual analysis or based on a small number of professionals in specific geographic regions. We conducted an online international survey of 279 UX practitioners on their practices and challenges while collaborating during data analysis. We found that UX practitioners were often under time pressure to conduct analysis and adopted three modes of collaboration: independently analyze different portions of the data and then collaborate, collaboratively analyze the session with little or no independent analysis, and independently analyze the same set of data and then collaborate. Moreover, most encountered challenges related to lack of resources, disagreements with?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3491102.3517647,8,kuang2022merging,"User experience,UX,Usability testing,Data analysis,Collaboration, Survey"
2022,In CHI Conference on Human Factors in Computing Systems (CHI'22),"""I Shake The Package To Check If It's Mine"": A Study of Package Fetching Practices and Challenges of Blind and Low Vision People in China","Wentao Lei, Mingming Fan, Juliann Thang",Wentao Lei,English,"With about 230 million packages delivered per day in 2020, fetching packages has become a routine for many city dwellers in China. When fetching packages, people usually need to go to collection sites of their apartment complexes or a KuaiDiGui, an increasingly popular type of self-service package pickup machine. However, little is known whether such processes are accessible to blind and low vision (BLV) city dwellers. We interviewed BLV people (N=20) living in a large metropolitan area in China to understand their practices and challenges of fetching packages. Our findings show that participants encountered difficulties in finding the collection site and localizing and recognizing their packages. When fetching packages from KuaiDiGuis, they had difficulty in identifying the correct KuaiDiGui, interacting with its touch screen, navigating the complex on-screen workflow, and opening the target compartment. We?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3491102.3502063,8,lei2022shake,"Package delivery, KuaiDiGui, Blind and low vision, People with vision impairments, Qualitative study, Interview, China, Accessibility "
2022,Proceedings of the 24th International ACM SIGACCESS Conference on Computers?¡­,"¡°I Used To Carry A Wallet, Now I Just Need To Carry My Phone¡±: Understanding Current Banking Practices and Challenges Among Older Adults in China","Xiaofu Jin, Mingming Fan",Xiaofu Jin,English,"Managing finances is crucial for older adults who are retired and may rely on savings to ensure their lives¡¯ quality. As digital banking platforms (e.g., mobile apps, electronic payment) gradually replace physical ones, it is critical to understand how they adapt to digital banking and the potential frictions they experience. We conducted semi-structured interviews with 16 older adults in China, where the aging population is the largest and digital banking grows fast. We also interviewed bank employees to gain complementary perspectives of these help givers. Our findings show that older adults used both physical and digital platforms as an ecosystem based on perceived pros and cons. Perceived usefulness, self-confidence, and social influence were key motivators for learning digital banking. They experienced app-related (e.g., insufficient error-recovery support) and user-related challenges (e.g., trust, security and?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3517428.3544820,7,jin2022used,"Older adults, elderly, seniors, aging, banking, virtual bank, electronic payment, mobile banking, accessibility, technology use, digital inclusion, digital equity "
2020,Proceedings of the 1st International Workshop on Human-Centric Multimedia?¡­,iWink: Exploring eyelid gestures on mobile devices,"Zhen Li, Mingming Fan, Ying Han, Khai N Truong",Zhen Li,English,"Although gaze has been widely studied for mobile interactions, eyelid-based gestures are relatively understudied and limited to few basic gestures (e.g., blink). In this work, we propose a gesture grammar to construct both basic and compound eyelid gestures. We present an algorithm to detect nine eyelid gestures in real-time on mobile devices and evaluate its performance with 12 participants. Results show that our algorithm is able to recognize nine eyelid gestures with 83% and 78% average accuracy using user-dependent and user-independent models respectively. Further, we design a gesture mapping scheme to allow for navigating between and within mobile apps only using eyelid gestures. Moreover, we show how eyelid gestures can be used to enable cross-application and sensitive interactions. Finally, we highlight future research directions.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3422852.3423479,7,li2020iwink,"Eyelid gestures, hands-free interaction, mobile interaction"
2024,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern?¡­,Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting,"Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang",Zhijing Shao,English,We present SplattingAvatar a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion we control the rotation and translation of the Gaussians directly by mesh which empowers its compatibility with various animation techniques eg skeletal animation blend shapes and mesh editing. Trainable from monocular videos for both full-body and head avatars SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.,Conference paper,https://openaccess.thecvf.com/content/CVPR2024/html/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.html,6,shao2024splattingavatar,None
2022,Interacting with Computers,Older Adults¡¯ Concurrent and Retrospective Think-Aloud Verbalizations for Identifying User Experience Problems of VR Games,"Mingming Fan, Vinita Tibdewal, Qiwen Zhao, Lizhou Cao, Chao Peng, Runxuan Shu, Yujia Shan",Mingming Fan,English," While virtual reality (VR) games are beneficial for older adults to improve their physical functions and cognitive abilities, VR research often does not include older adults. Our review of the proceedings of major HCI conferences (i.e. ASSETS, CHI, CHI PLAY, CSCW and DIS) between 2016 and 2020 shows that only three out of 352 VR-related papers involved older adults. Consequently, older adults tend to encounter user experience (UX) problems with VR. One common way to identify UX problems is to conduct usability testing with think-aloud (TA) protocols. As VR games tend to be perceptually and physically demanding, older adults might need to allocate more resources to VR content and interaction and thus have fewer resources for thinking aloud. This raises the question of whether TA protocols are still a viable approach to detecting UX problems of VR games for older adult participants. To answer this?¡­",Article,https://academic.oup.com/iwc/article-abstract/34/4/99/6967130,6,fan2022older,"older adults, think-aloud protocols, virtual reality, VR games, verbalization, UX problems,user experience"
2024,International Journal of Human¨CComputer Interaction,PoeticAR: Reviving traditional poetry of the heritage site of jichang garden via augmented reality,"Jin Tian, Yifan Cao, Lingyi Feng, Dongting Fu, Linping Yuan, Huamin Qu, Yang Wang, Mingming Fan",Jin Tian,English,"As a famed Chinese classical garden, the Jichang Garden was a constant inspiration to many poets in its hundreds of years¡¯ history, who composed a rich body of poems¡ªa valuable intangible cultural heritage. While tourists tend to pay attention to tangible natural scenery and historical architectures, they often neglect intangible cultural heritage¡ªpoems. We interviewed 23 tourists and found that augmented reality (AR) was viable for tourists to enjoy the physical scenery and the poetry simultaneously. We developed an initial prototype of PoeticAR, which presents poems based on physical scenery to enhance tourists¡¯ cultural and aesthetic experience. We further revised the prototype based on the ideas generated from a workshop with 18 tourists. We conducted a between-subject user study with 30 tourists to compare PoeticAR with Video. Results showed that PoeticAR significantly motivated tourists¡¯ interest in?¡­",Article,https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2176806,5,tian2024poeticar,
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,Sparkling silence: Practices and challenges of livestreaming among deaf or hard of hearing streamers,"Beiyan Cao, Changyang He, Muzhi Zhou, Mingming Fan",Beiyan Cao,English,"Understanding livestream platforms¡¯ accessibility challenges for minority groups, such as people with disabilities, is critical to increasing the diversity and inclusion of those platforms. While prior work investigated the experiences of streamers with vision or motor loss, little is known about the experiences of deaf or hard of hearing (DHH) streamers who must work with livestreaming platforms that heavily depend on audio. We conducted semi-structured interviews with DHH streamers to learn why they livestream, how they navigate livestream platforms and related challenges. Our findings revealed their desire to break the stereotypes towards the DHH groups via livestream and the intense interplay between interaction methods, such as sign language, texts, lip language, background music, and viewer characteristics. Major accessibility challenges include the lack of real-time captioning, the small sign language?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581053,5,cao2023sparkling,"deaf or hard of hearing (DHH), livestreaming, accessibility, social media/online communities "
2023,arXiv preprint arXiv:2310.09235,CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming,"Felicia Li Feng, Ryan Yen, Yuzhe You, Mingming Fan, Jian Zhao, Zhicong Lu",Felicia Li Feng,English,"Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators' progress and intents. In this paper, we aim to investigate ways to assist programmers' prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators' prompts and building on their collaborators' work, reducing repetitive updates and communication costs.",Article,https://arxiv.org/abs/2310.09235,4,feng2023coprompt,"large language model, collaborative programming, prompt engineering, natural language programming, natural language interface"
2023,Proceedings of the 25th International ACM SIGACCESS Conference on Computers?¡­,Understanding Curators' Practices and Challenge of Making Exhibitions More Accessible for People with Visual Impairments,"Yuru Huang, Jingling Zhang, Xiaofu Jin, Mingming Fan",Yuru Huang,English," Assistive technologies are increasingly developed and applied in exhibition environments to help blind and low vision (BLV) people deal with the challenges they face when visiting exhibitions. While studies have examined the experiences of BLV people using such technologies, little is known about the experiences and challenges of curators incorporating assistive technologies into exhibitions to make them more accessible to BLV people. This research focuses on assistive technologies for BLV people in exhibitions from a curatorial perspective. We conducted semi-structured interviews with twenty-two experienced curators to understand their practices and challenges. We also curated a list of assistive technologies from published papers and used them as probes to seek curators¡¯ attitudes and perceptions of such technologies. We uncovered four critical themes related to curators¡¯ challenges of making?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3597638.3608384,2,huang2023understanding,"Exhibition, museum, curator, blind, low vision, visually impaired, visual impairment, assistive technology, accessibility, probe study, interview "
2023,Proceedings of the 25th International ACM SIGACCESS Conference on Computers?¡­,Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low-vision People,"Chutian Jiang, Wentao Lei, Emily Kuang, Teng Han, Mingming Fan",Chutian Jiang,English," Being able to analyze and derive insights from data, which we call Daily Data Analysis (DDA), is an increasingly important skill in everyday life. While the accessibility community has explored ways to make data more accessible to blind and low-vision (BLV) people, little is known about how BLV people perform DDA. Knowing BLV people¡¯s strategies and challenges in DDA would allow the community to make DDA more accessible to them. Toward this goal, we conducted a mixed-methods study of interviews and think-aloud sessions with BLV people (N=16). Our study revealed five key approaches for DDA (i.e., overview obtaining, column comparison, key statistics identification, note-taking, and data validation) and the associated challenges. We discussed the implications of our findings and highlighted potential directions to make DDA more accessible for BLV people.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3597638.3608423,2,jiang2023understanding,"blind and low vision, BLV, qualitative study, data exploration, daily data analysis, DDA, interview, think-aloud, data accessibility "
2023,Proceedings of the ACM on Interactive,Exploring the Opportunities of AR for Enriching Storytelling with Family Photos between Grandparents and Grandchildren,"Zisu Li, Li Feng, Chen Liang, Yuru Huang, Mingming Fan",Zisu Li,English,"Storytelling with family photos, as an important mode of reminiscence-based activities, can be instrumental in promoting intergenerational communication between grandparents and grandchildren by strengthening generation bonds and shared family values. Motivated by challenges that existing technology approaches encountered for improving intergenerational storytelling (e.g., the need to hold the tablet, the potential view detachment from the physical world in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality (AR) to support intergenerational storytelling, which offers new capabilities (e.g., 3D models, new interactivity) to enhance the expression for the storyteller. We conducted a two-part exploratory study, where pairs of grandparents and grandchildren 1) participated in an in-person storytelling activity with a semi-structured interview 2) and then a participatory design session with AR?¡­",Article,https://dl.acm.org/doi/abs/10.1145/3610903,2,li2023exploring,"augmented reality, older adults, smartphone exploration, independent learning"
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,"CoPracTter: Toward Integrating Personalized Practice Scenarios, Timely Feedback and Social Support into An Online Support Tool for Coping with Stuttering in China","Li Feng, Zeyu Xiong, Xinyi Li, Mingming Fan",Li Feng,English," Stuttering is a speech disorder influencing over 70 million people worldwide, including 13 million in China. It causes low self-esteem among other detrimental effects on people who stutter (PwS). Although prior work has explored approaches to assist PwS, they primarily focused on western contexts. In our formative study, we found unique practices and challenges among Chinese PwS. We then iteratively designed an online tool, CoPracTter, to support Chinese PwS practicing speaking fluency with 1) targeted stress-inducing practice scenarios, 2) real-time speech indicators, and 3) personalized timely feedback from the community. We further conducted a seven-day deployment study (N=11) to understand how participants utilized these key features. To our knowledge, it is the first time such a prototype was designed and tested for a long time with multiple PwS participants online simultaneously. Results indicate?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581309,2,feng2023copractter,"People who stutter, Accessibility, Field Study, Assistive Technology "
2022,Proceedings of the ACM on Human-Computer Interaction,Typist Experiment: an Investigation of Human-to-Human Dictation via Role-play to Inform Voice-based Text Authoring,"Can Liu, Siying Hu, Li Feng, Mingming Fan",Can Liu,English,"Voice dictation is increasingly used for text entry, especially in mobile scenarios. However, the speech-based experience gets disrupted when users must go back to a screen and keyboard to review and edit the text. While existing dictation systems focus on improving transcription and error correction, little is known about how to support speech input for the entire text creation process, including composition, reviewing and editing. We conducted an experiment in which ten pairs of participants took on the roles of authors and typists to work on a text authoring task. By analysing the natural language patterns of both authors and typists, we identified new challenges and opportunities for the design of future dictation interfaces, including the ambiguity of human dictation, the differences between audio-only and with screen, and various passive and active assistance that can potentially be provided by future systems.",Article,https://dl.acm.org/doi/abs/10.1145/3555758,2,liu2022typist,"dictation, speech, text input, authoring, role-play, intelligent interface"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing,"Emily Kuang, Minghao Li, Mingming Fan, Kristen Shinohara",Emily Kuang,English," Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642168,1,kuang2024enhancing,"User experience, Usability testing, Human-AI collaboration, Proactive conversational assistants "
2024,arXiv preprint arXiv:2403.09326,HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation,"Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, Xiaohang Zhan, Zeyu Wang",Duotun Wang,English,"We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.",Article,https://arxiv.org/abs/2403.09326,1,wang2024headevolver,None
2024,arXiv preprint arXiv:2402.04991,Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults Explore and Learn Smartphone Applications,"Xiaofu Jin, Wai Tong, Xiaoying Wei, Xian Wang, Emily Kuang, Xiaoyu Mo, Huamin Qu, Mingming Fan",Xiaofu Jin,English,"The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults' smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR's effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.",Article,https://arxiv.org/abs/2402.04991,1,jin2024exploring,
2023,Proceedings of the Eleventh International Symposium of Chinese CHI,OperARtistry: An AR-based Interactive Application to Assist the Learning of Chinese Traditional Opera (Xiqu) Makeup,"Zeyu Xiong, Shihan Fu, Mingming Fan",Zeyu Xiong,English," Chinese Traditional Opera (Xiqu) is an important type of intangible cultural heritage and one key characteristic of Xiqu is its visual effects on face achieved via makeup. However, Xiqu makeup process, especially the eye-area makeup process, is complex and time-consuming, which poses a learning challenge for potential younger inheritors. We introduce OperARtistry, an interactive application based on Augmented Reality (AR) that offers in-situ Xiqu makeup guidance for beginners. Our application provides a step-by-step guide for Xiqu eye-area makeup, incorporating AR effects at each stage. Furthermore, we conducted an initial user study (n=6) to compare our approach with existing video-based tutorials to assess the effectiveness and usefulness of our approach. Our findings show that OperARtisty helped participants achieve high-quality eye-area makeup effects with less learning time.",Conference paper,https://dl.acm.org/doi/abs/10.1145/3629606.3629622,1,xiong2023operartistry,"Assistive Technology, User-Centered Design, Intangible Cultural Heritage, Augmented Reality, Computer Aided Instruction"
2023,Proceedings of the 36th Annual ACM Symposium on User Interface Software and?¡­,ShadowTouch: Enabling Free-Form Touch-Based Hand-to-Surface Interaction with Wrist-Mounted Illuminant by Shadow Projection,"Chen Liang, Xutong Wang, Zisu Li, Chi Hsia, Mingming Fan, Chun Yu, Yuanchun Shi",Chen Liang,English," We present ShadowTouch, a novel sensing method to recognize the subtle hand-to-surface touch state for independent fingers based on optical auxiliary. ShadowTouch mounts a forward-facing light source on the user¡¯s wrist to construct shadows on the surface in front of the fingers when the corresponding fingers are close to the surface. With such an optical design, the subtle vertical movements of near-surface fingers are magnified and turned to shadow features cast on the surface, which are recognizable for computer vision algorithms. To efficiently recognize the touch state of each finger, we devised a two-stage CNN-based algorithm that first extracted all the fingertip regions from each frame and then classified the touch state of each region from the cropped consecutive frames. Evaluations showed our touch state detection algorithm achieved a recognition accuracy of 99.1% and an F-1 score of 96.8% in the?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3586183.3606785,1,liang2023shadowtouch,"touch detection, hand-to-surface interaction, computer vision "
2023,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems?¡­,Enhancing Older Adults¡¯ Gesture Typing Experience Using the T9 Keyboard on Small Touchscreen Devices,"Emily Kuang, Ruihuan Chen, Mingming Fan",Emily Kuang,English," Older adults increasingly adopt small-screen devices, but limited motor dexterity hinders their ability to type effectively. While a 9-key (T9) keyboard allocates larger space to each key, it is shared by multiple consecutive letters. Consequently, users must interrupt their gestures when typing consecutive letters, leading to inefficiencies and poor user experience. Thus, we proposed a novel keyboard that leverages the currently unused key 1 to duplicate letters from the previous key, allowing the entry of consecutive letters without interruptions. A user study with 12 older adults showed that it significantly outperformed the T9 with wiggle gesture in typing speed, KSPC, insertion errors, and deletes per word while achieving comparable performance as the conventional T9. Repeating the typing tasks with 12 young adults found that the advantages of the novel T9 were consistent or enhanced. We also provide error analysis?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3544548.3581105,1,kuang2023enhancing,"Gesture Typing, Text Entry, Small Touchscreen  Devices, Older Adults, T9 Keyboard "
2023,Proceedings of the 28th International Conference on Intelligent User?¡­,SmartRecorder: An IMU-based Video Tutorial Creation by Demonstration System for Smartphone Interaction Tasks,"Xiaozhu Hu, Yanwen Huang, Bo Liu, Ruolan Wu, Yongquan Hu, Aaron J Quigley, Mingming Fan, Chun Yu, Yuanchun Shi",Xiaozhu Hu,English," This work focuses on an active topic in the HCI community, namely tutorial creation by demonstration. We present a novel tool named SmartRecorder that facilitates people, without video editing skills, creating video tutorials for smartphone interaction tasks. As automatic interaction trace extraction is a key component to tutorial generation, we seek to tackle the challenges of automatically extracting user interaction traces on smartphones from screencasts. Uniquely, with respect to prior research in this field, we combine computer vision techniques with IMU-based sensing algorithms, and the technical evaluation results show the importance of smartphone IMU data in improving system performance. With the extracted key information of each step, SmartRecorder generates instructional content initially and provides tutorial creators with a tutorial refinement editor designed based on a high recall (99.38%) of key steps?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3581641.3584069,1,hu2023smartrecorder,"Tutorial Creation by Demonstration, IMU-based Interaction Trace Extraction, Non-experts, Smartphone Usage Video Tutorial"
2021,Proceedings of the Ninth International Symposium of Chinese CHI,Think-Aloud Verbalizations for Identifying User Experience Problems: Effects of Language Proficiency with Chinese Non-Native English Speakers,"Mingming Fan, Lingyun Zhu",Mingming Fan,English,"Subtle patterns in users¡¯ think-aloud (TA) verbalizations (i.e., utterances) are shown to be telltale signs of user experience (UX) problems and used to build artificial intelligence (AI) models or AI-assisted tools to help UX evaluators identify UX problems automatically or semi-automatically. Despite the potential of such verbalization patterns, they were uncovered with native English speakers. As most people who speak English are non-native speakers, it is important to investigate whether similar patterns exist in non-native English speakers¡¯ TA verbalizations. As a first step to answer this question, we conducted think-aloud usability testing with Chinese non-native English speakers and native English speakers using three common TA protocols. We compared their verbalizations and UX problems that they encountered to understand the effects of language and TA protocols. Our findings show that both language?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3490355.3490358,1,fan2021think,"language proficiency, think aloud protocols, verbalization, Chinese non-native English speakers"
2021,arXiv preprint arXiv:2102.10685,EvoK: Connecting loved ones through Heart Rate sharing,"Esha Shandilya, Yiwen Wang, Xuan Zhao, Mingming Fan",Esha Shandilya,English,"In this work, we present EvoK, a new way of sharing one's heart rate with feedback from their close contacts to alleviate social isolation and loneliness. EvoK consists of a pair of wearable prototype devices (i.e., sender and receiver). The sender is designed as a headband enabling continuous sensing of heart rate with aesthetic designs to maximize social acceptance. The receiver is designed as a wristwatch enabling unobtrusive receiving of the loved one's continuous heart rate with multi-modal notification systems.",Article,https://arxiv.org/abs/2102.10685,1,shandilya2021evok,"Emotion sharing, heart rate, social isolation, mental health, wearable device"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Neural Canvas: Supporting Scenic Design Prototyping by Integrating 3D Sketching and Generative AI,"Yulin Shen, Yifei Shen, Jiawen Cheng, Chutian Jiang, Mingming Fan, Zeyu Wang",Yulin Shen,English," We propose Neural Canvas, a lightweight 3D platform that integrates sketching and a collection of generative AI models to facilitate scenic design prototyping. Compared with traditional 3D tools, sketching in a 3D environment helps designers quickly express spatial ideas, but it does not facilitate the rapid prototyping of scene appearance or atmosphere. Neural Canvas integrates generative AI models into a 3D sketching interface and incorporates four types of projection operations to facilitate 2D-to-3D content creation. Our user study shows that Neural Canvas is an effective creativity support tool, enabling users to rapidly explore visual ideas and iterate 3D scenic designs. It also expedites the creative process for both novices and artists who wish to leverage generative AI technology, resulting in attractive and detailed 3D designs created more efficiently than using traditional modeling tools or individual generative?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642096,0,shen2024neural,"generative AI, 3D sketching, scenic design, prototyping"
2024,Extended Abstracts of the CHI Conference on Human Factors in Computing?¡­,AromaBlendz: An Olfactory System for Crafting Personalized Scents,"Shihan Fu, Jianhao Chen, Yi Cai, Mingming Fan",Shihan Fu,English," Although the HCI community has recently begun to explore the usage of scent to enrich interactive system experiences (e.g., making VR more immersive), scent is often preset. In contrast, personalized scents might help trigger emotional responses and memory recall in many application scenarios, ranging from fostering relaxation to managing emotional states. We present AromaBlendz, a novel digital platform that enables users to create and customize their unique scent profiles. AromaBlendz comprises both hardware and software components that collectively deliver a seamless scent customization experience. The hardware includes a blending mechanism for essence oils and a user-friendly control unit, while the software component provides an intuitive interface for users to create, preview, and store their preferred scents. The platform not only allows for the generation of personalized scent profiles using a?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613905.3648670,0,fu2024aromablendz,"Olfactory Device, Hardware, Digital smell technology, Odour interfaces, Olfactory experiences"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts,"Chutian Jiang, Yinan Fan, Junan Xie, Emily Kuang, Kaihao Zhang, Mingming Fan",Chutian Jiang,English,"Charts are crucial in conveying information across various fields but are inaccessible to blind and low vision (BLV) people without assistive technology. Chart comprehension tools leveraging haptic feedback have been used widely but are often bulky, expensive, and static, rendering them inefficient for conveying chart data. To increase device portability, enable multitasking, and provide efficient assistance in chart comprehension, we introduce a novel system that delivers unobtrusive modulated electrotactile feedback directly to the fingertip edge. Our three-part study with twelve participants confirmed the effectiveness of this system, demonstrating that electrotactile feedback, when applied for 0.5 seconds with a 0.12-second interval, provides the most accurate position and direction recognition. Furthermore, our electrotactile device has proven valuable in assisting BLV participants in comprehending four commonly?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642546,0,jiang2024designing,"Accessibility, Electrotactile, Haptic Data Visualization, Accessible Data Visualization"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming,"Shihan Fu, Jianhao Chen, Emily Kuang, Mingming Fan",Shihan Fu,English," Literacy¡ªthe ability to read, write, and comprehend text¡ªis an important topic addressed by UNESCO. Despite global efforts to promote adult literacy education, rural areas with limited resources still lag behind. As livestreaming has gained popularity in China, many streamers leveraged its accessibility and affordability to reach low-literate adults. To gain a better understanding of the practices and challenges faced by adult literacy education through livestreaming, we conducted a mixed-methods study involving a 7-day observation of livestreaming sessions and an interview study with twelve streamers and ten viewers. We discovered streamers¡¯ altruistic motives and unique interactive approaches. Viewers perceived livestreaming as a more engaging, community-supportive method than traditional approaches. We also identified both shared and unique challenges for streamers and viewers that limit its efficacy as?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642423,0,fu2024bridging,"livestreaming, adult literacy, education "
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,WieldingCanvas: Interactive Sketch Canvases for Freehand Drawing in VR,"Xiaohui Tan, Zhenxuan He, Can Liu, Mingming Fan, Tianren Luo, Zitao Liu, Mi Tian, Teng Han, Feng Tian",Xiaohui Tan,English," Sketching in Virtual Reality (VR) is challenging mainly due to the absence of physical surface support and virtual depth perception cues, which induce high cognitive and sensorimotor load. This paper presents WieldingCanvas, an interactive VR sketching platform that integrates canvas manipulations to draw lines and curves in 3D. Informed by real-life examples of two-handed creative activities, WieldingCanvas interprets users¡¯ spatial gestures to move, swing, rotate, transform, or fold a virtual canvas, whereby users simply draw primitive strokes on the canvas, which are turned into finer and more sophisticated shapes via the manipulation of the canvas. We evaluated the capability and user experience of WieldingCanvas with two studies where participants were asked to sketch target shapes. A set of freehand sketches of high aesthetic qualities were created, and the results demonstrated that WieldingCanvas can?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642047,0,tan2024wieldingcanvas,
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,CharacterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars,"Hua Xuan Qin, Shan Jin, Ze Gao, Mingming Fan, Pan Hui",Hua Xuan Qin,English," Support for story character construction is as essential as characters are for stories. Building upon past research on early character construction stages, we explore how conversation with chatbot avatars embodying characters powered by more recent technologies could support the entire character construction process for creative writing. Through a user study (N=14) with creative writers, we examine thinking and usage patterns of CharacterMeet, a prototype system allowing writers to progressively manifest characters through conversation while customizing context, character appearance, voice, and background image. We discover that CharacterMeet facilitates iterative character construction. Specifically, participants, including those with more linear usual approaches, alternated between writing and personalized exploration through visualization of ideas on CharacterMeet while visuals and audio enhanced?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642105,0,qin2024charactermeet,"Creativity Support, Writing Assistants, Creative Writing, Human- AI Collaboration, Large Language Models "
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR,"Jingze Tian, Yingna Wang, Keye Yu, Liyi Xu, Junan Xie, Franklin Mingzhe Li, Yafeng Niu, Mingming Fan",Jingze Tian,English,"Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants¡¯ abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642884,0,tian2024designing,"people with spinal muscular atrophy, virtual reality, upper-body gestures, user-defined gestures"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults,"Qiuxin Du, Zhen Song, Haiyan Jiang, Xiaoying Wei, Dongdong Weng, Mingming Fan",Qiuxin Du,English," The decline of cognitive inhibition significantly impacts older adults¡¯ quality of life and well-being, making it a vital public health problem in today¡¯s aging society. Previous research has demonstrated that Virtual reality (VR) exergames have great potential to enhance cognitive inhibition among older adults. However, existing commercial VR exergames were unsuitable for older adults¡¯ long-term cognitive training due to the inappropriate cognitive activation paradigm, unnecessary complexity, and unbefitting difficulty levels. To bridge these gaps, we developed a customized VR cognitive training exergame (LightSword) based on Dual-task and Stroop paradigms for long-term cognitive inhibition training among healthy older adults. Subsequently, we conducted an eight-month longitudinal user study with 12 older adults aged 60 years and above to demonstrate the effectiveness of LightSword in improving cognitive?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642187,0,du2024lightsword,"Virtual Reality, Health, Older Adults "
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,¡°Can It Be Customized According to My Motor Abilities?¡±: Toward Designing User-Defined Head Gestures for People with Dystonia,"Qin Sun, Yunqi Hu, Mingming Fan, Jingting Li, Su-Jing Wang",Qin Sun,English," Recent studies proposed above-the-neck gestures for people with upper-body motor impairments interacting with mobile devices without finger touch, resulting in an appropriate user-defined gesture set. However, many gestures involve sustaining eyelids in closed or open states for a period. This is challenging for people with dystonia, who have difficulty sustaining and intermitting muscle contractions. Meanwhile, other facial parts, such as the tongue and nose, can also be used to alleviate the sustained use of eyes in the interaction. Consequently, we conducted a user study inviting 16 individuals with dystonia to design gestures based on facial muscle movements for 26 common smartphone commands. We collected 416 user-defined head gestures involving facial features and shoulders. Finally, we obtained the preferred gestures set for individuals with dystonia. Participants preferred to make the gestures with?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642378,0,sun2024can,"Dystonia, Gesture interaction, Interaction technology, Interaction preferences, Human-computer interaction"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation,"Peixuan Xiong, Yukai Zhang, Nandi Zhang, Shihan Fu, Xin Li, Yadan Zheng, Jinni Zhou, Xiquan Hu, Mingming Fan",Peixuan Xiong,English," Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts. Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes. Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence. While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored. In this paper, we present a VR rehabilitation application that harnesses hand redirection. Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642912,0,xiong2024reach,"Motor impairments, Upper limb rehabilitation, Virtual hand redi- rection "
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effect on Target Selection and Manipulation Tasks in VR,"Zhiqing Wu, Duotun Wang, Shumeng Zhang, Yuru Huang, Zeyu Wang, Mingming Fan",Zhiqing Wu,English,"Recent studies show the promise of VR in improving physical, cognitive, and emotional health of older adults. However, prior work on optimizing object selection and manipulation performance in VR was mostly conducted among younger adults. It remains unclear how older adults would perform such tasks compared to younger adults and the challenges they might face. To fill in this gap, we conducted two studies with both older and younger adults to understand their performances and user experiences of object selection and manipulation in VR respectively. Based on the results, we delineated interaction difficulties that older adults exhibited in VR and identified multiple factors, such as headset-related neck fatigue, extra head movements from out-of-view interactions, and slow spatial perceptions, that significantly decreased the motor performance of older adults. We further proposed design recommendations for?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642558,0,wu2024toward,"Virtual/Augmented Reality, Older Adults, Empirical study that tells us about people, Lab Study"
2024,Proceedings of the CHI Conference on Human Factors in Computing Systems,"See Widely, Think Wisely: Toward Designing a Generative Multi-agent System to Burst Filter Bubbles","Yu Zhang, Jingwei Sun, Li Feng, Cen Yao, Mingming Fan, Liuxin Zhang, Qianying Wang, Xin Geng, Yong Rui",Yu Zhang,English," The proliferation of AI-powered search and recommendation systems has accelerated the formation of ¡°filter bubbles¡± that reinforce people¡¯s biases and narrow their perspectives. Previous research has attempted to address this issue by increasing the diversity of information exposure, which is often hindered by a lack of user motivation to engage with. In this study, we took a human-centered approach to explore how Large Language Models (LLMs) could assist users in embracing more diverse perspectives. We developed a prototype featuring LLM-powered multi-agent characters that users could interact with while reading social media content. We conducted a participatory design study with 18 participants and found that multi-agent dialogues with gamification incentives could motivate users to engage with opposing viewpoints. Additionally, progressive interactions with assessment tasks could promote?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3613904.3642545,0,zhang2024see,"filter bubble, multi-agent system, large language model, interaction design, diverse information "
2024,IEEE Transactions on Visualization and Computer Graphics,VisTellAR: Embedding Data Visualization to Short-form Videos Using Mobile Augmented Reality,"Wai Tong, Kento Shigyo, Lin-Ping Yuan, Mingming Fan, Ting-Chuen Pong, Huamin Qu, Meng Xia",Wai Tong,English,"With the rise of short-form video platforms and the increasing availability of data, we see the potential for people to share short-form videos embedded with data in situ (e.g., daily steps when running) to increase the credibility and expressiveness of their stories. However, creating and sharing such videos in situ is challenging since it involves multiple steps and skills (e.g., data visualization creation and video editing), especially for amateurs. By conducting a formative study (N=10) using three design probes, we collected the motivations and design requirements. We then built VisTellAR, a mobile AR authoring tool, to help amateur video creators embed data visualizations in short-form videos in situ. A two-day user study shows that participants (N=12) successfully created various videos with data visualizations in situ and they confirmed the ease of use and learning. AR pre-stage authoring was useful to assist people?¡­",Article,https://ieeexplore.ieee.org/abstract/document/10457053/,0,tong2024vistellar,
2024,arXiv preprint arXiv:2402.15719,""" It Is Hard to Remove from My Eye"": Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers","Zeyu Xiong, Shihan Fu, Yanying Zhu, Chenqing Zhu, Xiaojuan Ma, Mingming Fan",Zeyu Xiong,English,"Chinese traditional opera (Xiqu) performers often experience skin problems due to the long-term use of heavy-metal-laden face paints. To explore the current skincare challenges encountered by Xiqu performers, we conducted an online survey (N=136) and semi-structured interviews (N=15) as a formative study. We found that incomplete makeup removal is the leading cause of human-induced skin problems, especially the difficulty in removing eye makeup. Therefore, we proposed EyeVis, a prototype that can visualize the residual eye makeup and record the time make-up was worn by Xiqu performers. We conducted a 7-day deployment study (N=12) to evaluate EyeVis. Results indicate that EyeVis helps to increase Xiqu performers' awareness about removing makeup, as well as boosting their confidence and security in skincare. Overall, this work also provides implications for studying the work of people who wear makeup on a daily basis, and helps to promote and preserve the intangible cultural heritage of practitioners.",Article,https://arxiv.org/abs/2402.15719,0,xiong2024hard,"makeup, Chinese traditional opera, computer vision, mobile computing, intangible cultural heritage,interactive design"
2024,arXiv preprint arXiv:2402.15723,"FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search?¡­","Zhitong Guan, Zeyu Xiong, Mingming Fan",Zhitong Guan,English,"Parcel lockers have become an increasingly prevalent last-mile delivery method. Yet, a recent study revealed its accessibility challenges to blind and low-vision people (BLV). Informed by the study, we designed FetchAid, a standalone intelligent mobile app assisting BLV in using a parcel locker in real-time by integrating computer vision and augmented reality (AR) technologies. FetchAid first uses a deep network to detect the user's fingertip and relevant buttons on the touch screen of the parcel locker to guide the user to reveal and scan the QR code to open the target compartment door and then guide the user to reach the door safely with AR-based context-aware audio feedback. Moreover, FetchAid provides an error-recovery mechanism and real-time feedback to keep the user on track. We show that FetchAid substantially improved task accomplishment and efficiency, and reduced frustration and overall effort in a study with 12 BLV participants, regardless of their vision conditions and previous experience.",Article,https://arxiv.org/abs/2402.15723,0,guan2024fetchaid,"Package delivery, KuaiDiGui, Blind and low vision, People with vision impairments, Accessibility, Mobile devices, Object detection, Computer vision, Augmented reality, Assistive technology"
2024,None,Exploring the Impact of Artificial Intelligence-Generated Content (AIGC) Tools on Social Dynamics in UX Collaboration,"Ziyan Wang, Luyao Shen, Emily Kuang, Shumeng Zhang, Mingming Fan",Ziyan Wang,English,"Artificial Intelligence-Generated Content (AIGC) tools have gradually been integrated into the daily workflow of UX practitioners. While existing research has explored the integration of AIGC tools in daily workflow, little is known about their impact on social dynamics within UX collaboration. We conducted four focus groups and eight semi-structured interviews with 26 UX practitioners to investigate how AIGC tools influence social dynamics in UX collaboration. Our findings indicated that AIGC tools not only mitigated conflicts but also introduced potential new conflicts. AIGC tools expanded the roles of UX practitioners and fostered a team culture characterized by exploring and discussing. Participants have higher expectations for AI-assisted design in user understanding and prototype evaluation, and team-motivated AI tools learning. Based on these findings, we discussed the benefits and concerns of conflict resolution through AIGC and the importance of teams in AI learning. Finally, we proposed several suggestions for future AI design research.",Conference paper,https://www.mingmingfan.com/papers/DIS24_AIGC_Social_Dynamics_UX.pdf,0,wang2024exploring,"user experience design, user interface design, AI-generated content, social dynamics, collaboration"
2023,Proceedings of the Eleventh International Symposium of Chinese CHI,Toward Leveraging Augmented Reality (AR) for Enhancing Remote Intergenerational Communication in Cooking Scenarios,"Yuru Huang, Liyi Xu, You Zhou, Qiongyan Chen, Zhiqing Wu, Li Feng, Mingming Fan",Yuru Huang,English," The close connection between food culture and family relationship has always been regarded as an important link to maintain family harmony and intergenerational(IG) communication. Through cooking and eating together, family members inherit family customs and culture, and enhance mutual feelings and understanding. However, with the development changes in family structure, there is an urgent need to find new approaches for IG communication between the younger and older generations. In this context, this study will explore how modern technological means such as remote home intelligent control and augmented reality (AR) technology, combined with the characteristics of food culture, can be used to innovate and strengthen IG communication between family members. This study explored the challenges inherent in remote integrated circuits in the context of collaborative cooking. Building on this, we use?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3629606.3629658,0,huang2023toward,"Intergenerational Communication, AR, Human-food Interaction"
2023,Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface?¡­,A Multi-modal Toolkit to Support DIY Assistive Technology Creation for Blind and Low Vision People,"Liwen He, Yifan Li, Mingming Fan, Liang He, Yuhang Zhao",Liwen He,English," We design and build A11yBits, a tangible toolkit that empowers blind and low vision (BLV) people to easily create personalized do-it-yourself assistive technologies (DIY-ATs). A11yBits includes (1) a series of Sensing modules to detect both environmental information and user commands, (2) a set of Feedback modules to send multi-modal feedback, and (3) two Base modules (Sensing Base and Feedback Base) to power and connect the sensing and feedback modules. The toolkit enables accessible and easy assembly via a ¡°plug-and-play¡± mechanism. BLV users can select and assemble their preferred modules to create personalized DIY-ATs. ",Conference paper,https://dl.acm.org/doi/abs/10.1145/3586182.3616646,0,he2023multi,"Accessibility, blind and low vision, DIY toolkit, tangible modules "
2023,Proceedings of the 31st ACM International Conference on Multimedia,Designing Loving-Kindness Meditation in Virtual Reality for Long-Distance Romantic Relationships,"Xian Wang, Xiaoyu Mo, Lik-Hang Lee, Xiaoying Wei, Xiaofu Jin, Mingming Fan, Pan Hui",Xian Wang,English,"Loving-kindness meditation (LKM) is used in clinical psychology for couples' relationship therapy, but physical isolation can make the relationship more strained and inaccessible to LKM. Virtual reality (VR) can provide immersive LKM activities for long-distance couples. However, no suitable commercial VR applications for couples exist to engage in LKM activities of long-distance. This paper organized a series of workshops with couples to build a prototype of a couple-preferred LKM app. Through analysis of participants' design works and semi-structured interviews, we derived design considerations for such VR apps and created a prototype for couples to experience. We conducted a study with couples to understand their experiences of performing LKM using the VR prototype and a traditional video conferencing tool. Results show that LKM session utilizing both tools has a positive effect on the intimate?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3581783.3612438,0,wang2023designing,"Couples, Meditation, Participatory Design, Virtual Reality"
2023,Proceedings of the 16th International Symposium on Visual Information?¡­,OdorV-Art: An Initial Exploration of An Olfactory Intervention for Appreciating Style Information of Artworks in Virtual Museum,"Shumeng Zhang, Ziyan Wang, You Zhou, Hao Cui, Shihan Fu, Zeyu Wang, Mingming Fan",Shumeng Zhang,English," Style information, such as tone, mood, and genre of artworks, is important for museum visitors to appreciate them better. However, such information can be challenging for non-art specialists to comprehend in the short period that they view artworks. The sense of smell is instrumental for humans to assist their image memory, color, emotion, and shape association. However, it is rarely used in the appreciation of artworks. Taking Western landscape painting as an example, this research explores the following research questions (RQs): 1) How does the intervention of the sense of smell improve the acquisition of style information in paintings? 2) How does the intervention of the sense of smell enhance the immersion in painting appreciation? To answer RQs, we first recruited seven art specialists to participate in a co-design workshop to design a prototype of the virtual museum with olfactory intervention. We then?¡­",Conference paper,https://dl.acm.org/doi/abs/10.1145/3615522.3615544,0,zhang2023odorv,"Virtual Reality, Virtual Museum, Olfactory Display, Art Appreciation, Landscape Painting"
